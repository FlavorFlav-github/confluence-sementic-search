# File: llm/gemini_adapter.py

import os
from typing import Any, Dict

from google import genai
from google.genai.errors import APIError

# Assuming LLMAdapter is defined in llm.base_adapter
from llm.base_adapter import LLMAdapter


class GeminiModelAdapter(LLMAdapter):
    """
    Concrete implementation of the LLMAdapter for the Gemini API backend.

    This adapter manages the connection, authentication, and core generation
    call to Google's Gemini models.
    """

    def __init__(self, search_system: Any, model_name: str):
        """
        Initializes the Gemini adapter.

        Args:
            search_system (Any): The retrieval/search system instance.
            model_name (str): The specific name of the model to use (e.g., 'gemini-2.5-flash').
        """
        super().__init__(search_system, model_name)
        self.client = None

        # Override system prompt. The content is passed as a system_instruction in the API call.
        self.system_prompt = self.system_prompt.strip()

    def setup(self) -> bool:
        """
        Initializes the Gemini client and verifies API key availability and model access.

        Returns:
            bool: True if setup was successful, False otherwise.
        """
        print(f"ðŸš€ Setting up Gemini API with model: {self.model_name}")

        # Check for API Key in environment variables
        if not os.getenv("GEMINI_API_KEY"):
            print("âŒ GEMINI_API_KEY environment variable not found.")
            print("Please set the key to proceed with the Gemini API adapter.")
            self.is_ready = False
            return False

        try:
            # The genai.Client() automatically picks up the GEMINI_API_KEY
            self.client = genai.Client()

            # CRITICAL FIX: Pass the model name as the positional argument.
            # This verifies the client can connect and the model name is recognized.
            model_info = self.client.models.get(model=self.model_name)

            print(f"âœ… Gemini Client initialized. Model: {model_info.name} available.")
            self.is_ready = True
            return True

        except (APIError, Exception) as e:
            print(f"âŒ Failed to set up Gemini model '{self.model_name}'. Error: {e}")
            self.is_ready = False
            return False

    def ask(self, prompt: str) -> str:
        """
        Public method for the Bridge to call for direct LLM generation.
        Delegates to the internal _generate method.
        """
        if not self.is_ready:
            raise RuntimeError(f"Gemini model '{self.model_name}' is not set up or ready.")

        return self._generate(prompt)

    def _generate(self, prompt: str) -> str:
        """
        Core implementation: Sends the prompt to the Gemini API.

        Args:
            prompt (str): The full, constructed RAG prompt (including context and question).

        Returns:
            str: The raw text response generated by the LLM.
        """

        try:
            # We use the system_prompt defined in LLMAdapter as the System Instruction
            # and the full RAG-formatted prompt as the user content.
            response = self.client.models.generate_content(
                model=self.model_name,
                contents=prompt,
                config={
                    "system_instruction": self.system_prompt,
                    "temperature": 0.2,
                    "max_output_tokens": 1024
                }
            )

            # The API returns an empty response if generation is blocked or fails,
            # so we handle that case.
            if not response.text:
                raise Exception("API returned an empty response. Possible content blocking or generation error.")

            return response.text.strip()

        except APIError as e:
            raise Exception(f"Gemini API Error: {e}")
        except Exception as e:
            raise Exception(f"Error during Gemini generation: {e}")
